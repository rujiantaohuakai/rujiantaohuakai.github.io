<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>梯度下降与最小二乘法求解线性回归 | Taohua's Blog</title><meta name="author" content="Taohua,m1773101654@outlook.com"><meta name="copyright" content="Taohua"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="线性回归形式化定义 梯度下降算法例子： 步骤： 代码实现： 12345678910111213141516171819202122232425262728import numpy as npimport matplotlib.pyplot as plt# 梯度下降法求解求y &#x3D; x^2 + 2x + 5# 画出函数图像x &#x3D; np.linspace(-6, 4, 100)y &#x3D; x**2 + 2*">
<meta property="og:type" content="article">
<meta property="og:title" content="梯度下降与最小二乘法求解线性回归">
<meta property="og:url" content="http://example.com/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/index.html">
<meta property="og:site_name" content="Taohua&#39;s Blog">
<meta property="og:description" content="线性回归形式化定义 梯度下降算法例子： 步骤： 代码实现： 12345678910111213141516171819202122232425262728import numpy as npimport matplotlib.pyplot as plt# 梯度下降法求解求y &#x3D; x^2 + 2x + 5# 画出函数图像x &#x3D; np.linspace(-6, 4, 100)y &#x3D; x**2 + 2*">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/music_cat.gif">
<meta property="article:published_time" content="2025-10-27T13:43:06.000Z">
<meta property="article:modified_time" content="2025-11-01T03:38:18.532Z">
<meta property="article:author" content="Taohua">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="学校课程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/music_cat.gif"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "梯度下降与最小二乘法求解线性回归",
  "url": "http://example.com/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/",
  "image": "http://example.com/img/music_cat.gif",
  "datePublished": "2025-10-27T13:43:06.000Z",
  "dateModified": "2025-11-01T03:38:18.532Z",
  "author": [
    {
      "@type": "Person",
      "name": "Taohua",
      "url": "http://example.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon_img.png"><link rel="canonical" href="http://example.com/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '梯度下降与最小二乘法求解线性回归',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(/img/background.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/music_cat.gif" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">17</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><span class="site-page group hide"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/algorithm/"><i class="fa-fw fas fa-book"></i><span> 算法</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/videos/"><i class="fa-fw fas fa-video"></i><span> 视频</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-file-code"></i><span> Demo</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/EldenRing/Home.html"><i class="fa-fw fas fa-ring"></i><span> EldenRing</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/top_default_img.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/music_cat.gif" alt="Logo"><span class="site-name">Taohua's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">梯度下降与最小二乘法求解线性回归</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><span class="site-page group hide"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/algorithm/"><i class="fa-fw fas fa-book"></i><span> 算法</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/videos/"><i class="fa-fw fas fa-video"></i><span> 视频</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-file-code"></i><span> Demo</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/EldenRing/Home.html"><i class="fa-fw fas fa-ring"></i><span> EldenRing</span></a></li></ul></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">梯度下降与最小二乘法求解线性回归</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-10-27T13:43:06.000Z" title="Created 2025-10-27 21:43:06">2025-10-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-11-01T03:38:18.532Z" title="Updated 2025-11-01 11:38:18">2025-11-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><h2 id="形式化定义"><a href="#形式化定义" class="headerlink" title="形式化定义"></a>形式化定义</h2><p><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/11.png"></p>
<h2 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h2><p>例子：<br><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/12.png"></p>
<p>步骤：<br><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/13.png"></p>
<p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度下降法求解求y = x^2 + 2x + 5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出函数图像</span></span><br><span class="line">x = np.linspace(-<span class="number">6</span>, <span class="number">4</span>, <span class="number">100</span>)</span><br><span class="line">y = x**<span class="number">2</span> + <span class="number">2</span>*x + <span class="number">5</span></span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机初始化x，seed用于固定随机数种子</span></span><br><span class="line">np.random.seed(<span class="number">1388</span>)</span><br><span class="line">x = np.random.randint(-<span class="number">100</span>, <span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;初始化x = &quot;</span>, x)</span><br><span class="line">alaph = <span class="number">0.8</span> <span class="comment"># 学习率</span></span><br><span class="line">iteraterNum = <span class="number">1000</span> <span class="comment"># 迭代次数</span></span><br><span class="line"><span class="comment"># 如果稳定(收敛)即可停止,所以可以计算本次迭代和上一次迭代的差值，在一定范围内即可break</span></span><br><span class="line"><span class="comment"># 也可以画损失函数图像，当两次损失之间相差不多时，也可以break</span></span><br><span class="line"><span class="comment"># y的导数为2x+2,迭代求theta</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iteraterNum):</span><br><span class="line">    <span class="comment"># 当前导数</span></span><br><span class="line">    derivative = <span class="number">2</span> * x + <span class="number">2</span></span><br><span class="line">    <span class="comment"># 更新最小值</span></span><br><span class="line">    x = x - alaph * derivative</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最终x = &quot;</span>, x)</span><br></pre></td></tr></table></figure>

<p>如果alpha太大，会导致迭代不收敛，如果alpha太小，会导致迭代过慢。</p>
<h3 id="批量梯度下降法"><a href="#批量梯度下降法" class="headerlink" title="批量梯度下降法"></a>批量梯度下降法</h3><p>使用所有样本来算梯度，然后更新参数<br>下降准确，但计算量大<br><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/15.png"></p>
<h3 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h3><p>仅仅随机使用一个样本来算梯度，然后更新参数<br>下降不准确，容易振荡下降<br><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/16.png"></p>
<h3 id="小批量梯度下降法"><a href="#小批量梯度下降法" class="headerlink" title="小批量梯度下降法"></a>小批量梯度下降法</h3><p>使用一小部分样本来算梯度，然后更新参数<br>前两种方法的折中，计算量小，下降较为准确<br><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/17.png"></p>
<h2 id="梯度下降法求解线性回归"><a href="#梯度下降法求解线性回归" class="headerlink" title="梯度下降法求解线性回归"></a>梯度下降法求解线性回归</h2><p>目标：使用梯度下降法求解，使得代价函数最小<br>J(θ)对θ求偏导数，得到θ的梯度，然后得到θ的迭代公式：θ &#x3D; θ - α * ∇J(θ)，其中α为学习率。m是样本数<br><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/14.png"></p>
<h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loaddata</span>():</span><br><span class="line">    <span class="comment"># 加载原始数据</span></span><br><span class="line">    data = np.loadtxt(<span class="string">&#x27;data\data1.txt&#x27;</span>, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    n = data.shape[<span class="number">1</span>] - <span class="number">1</span>  <span class="comment"># 特征的数量,shape[1]表示总列数</span></span><br><span class="line">    X = data[:, <span class="number">0</span>:n]  <span class="comment"># 取前n列作为特征</span></span><br><span class="line">    <span class="comment"># print(X) 列向量</span></span><br><span class="line">    y = data[:, -<span class="number">1</span>].reshape(-<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 取最后一列作为标签</span></span><br><span class="line">    <span class="comment"># print(y) 列向量</span></span><br><span class="line">    <span class="keyword">return</span> X, y</span><br></pre></td></tr></table></figure>

<h3 id="特征归一化"><a href="#特征归一化" class="headerlink" title="特征归一化"></a>特征归一化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">featureNormalize</span>(<span class="params">X</span>):</span><br><span class="line">    x_average = np.average(X, axis=<span class="number">0</span>)  <span class="comment"># 每一列的均值</span></span><br><span class="line">    sigma = np.std(X, axis=<span class="number">0</span>, ddof=<span class="number">1</span>)  <span class="comment"># 每一列的标准差,ddof=1表示除以n-1</span></span><br><span class="line">    X = (X - x_average) / sigma</span><br><span class="line">    <span class="keyword">return</span> X, x_average, sigma</span><br></pre></td></tr></table></figure>

<h3 id="计算代价J-θ"><a href="#计算代价J-θ" class="headerlink" title="计算代价J(θ)"></a>计算代价J(θ)</h3><p>假设函数：<br><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/02.png"><br>代价函数：<br><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/03.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">computeCost</span>(<span class="params">X, y, theta</span>):</span><br><span class="line">    m = X.shape[<span class="number">0</span>]  <span class="comment"># 行数,数据量</span></span><br><span class="line">    <span class="comment"># 计算代价</span></span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(np.power(np.dot(X, theta) - y, <span class="number">2</span>)) / (<span class="number">2</span> * m)</span><br></pre></td></tr></table></figure>

<h3 id="梯度下降函数"><a href="#梯度下降函数" class="headerlink" title="梯度下降函数"></a>梯度下降函数</h3><p><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/01.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果axis=0，则沿着纵轴进行操作</span></span><br><span class="line"><span class="comment"># 如果axis=1，则沿着横轴进行操作</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradientDescent</span>(<span class="params">X, y, theta, iterations, alpha</span>):</span><br><span class="line">    c = np.ones(X.shape[<span class="number">0</span>]).transpose() <span class="comment"># 生成与X行数相等的1,并转置变为列向量</span></span><br><span class="line">    X = np.insert(X, <span class="number">0</span>, values = c, axis = <span class="number">1</span>) <span class="comment"># 在横轴第0个位置插入c</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>] <span class="comment"># 行数,数据量</span></span><br><span class="line">    n = X.shape[<span class="number">1</span>] <span class="comment"># 列数, 特征数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存每次迭代的损失</span></span><br><span class="line">    costs = np.zeros(iterations)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="comment"># 第j个特征的新的theta = 原theta + alpha/m * sum((真实值-预测值) * 该特征的值)</span></span><br><span class="line">            <span class="comment"># 使用np.dot(X, theta)一次性计算出所有样本的预测值</span></span><br><span class="line">            <span class="comment"># 再用y-预测值计算出每个样本的误差</span></span><br><span class="line">            <span class="comment"># 再用每个样本的误差乘以该特征的值，最后求和得到该特征的梯度，然后更新theta</span></span><br><span class="line">            theta[j] = theta[j] + (alpha / m) * np.<span class="built_in">sum</span>((y - np.dot(X, theta)) * X[:, j].reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        costs[num] = computeCost(X, y, theta)</span><br><span class="line">    <span class="comment"># 返回theta和损失值</span></span><br><span class="line">    <span class="keyword">return</span> theta, costs</span><br></pre></td></tr></table></figure>

<h3 id="预测函数"><a href="#预测函数" class="headerlink" title="预测函数"></a>预测函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">X_predict, x_average, sigma, theta</span>):</span><br><span class="line">    <span class="comment"># 先对要预测的数据进行归一化</span></span><br><span class="line">    X_predict = (X_predict - x_average) / sigma</span><br><span class="line">    c = np.ones(X_predict.shape[<span class="number">0</span>]).transpose()</span><br><span class="line">    X_predict = np.insert(X_predict, <span class="number">0</span>, values = c, axis = <span class="number">1</span>)</span><br><span class="line">    X_predict_result = np.dot(X_predict, theta)</span><br><span class="line">    <span class="keyword">return</span> X_predict_result</span><br></pre></td></tr></table></figure>

<h3 id="模型评价函数"><a href="#模型评价函数" class="headerlink" title="模型评价函数"></a>模型评价函数</h3><p><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/18.png"><br>这三个值都是越小越好，越小表示模型的预测能力越好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># y_true为真实值，y_pred为模型预测值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mse</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 均方误差</span></span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>((np.power(y_true - y_pred, <span class="number">2</span>))) / <span class="built_in">len</span>(y_true)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rmse</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 均方根误差</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(np.<span class="built_in">sum</span>(np.power(y_true - y_pred, <span class="number">2</span>)) / <span class="built_in">len</span>(y_true))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mae</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 平均绝对误差</span></span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(y_true - y_pred)) / <span class="built_in">len</span>(y_true)</span><br></pre></td></tr></table></figure>

<h3 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    X_original, y = loaddata()</span><br><span class="line">    X ,x_average, sigma = featureNormalize(X_original)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化theta全为0，先生成长度为(X.shape[1] + 1)的全0数组，最后reshape为(X.shape[1] + 1,)</span></span><br><span class="line">    <span class="comment"># theta = np.zeros(X.shape[1] + 1).reshape(-1, 1) # -1表示自动推断数组长度</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 也可以在[-3,3]之间随机生成theta，指定数组形状为(X.shape[1] + 1, 1)</span></span><br><span class="line">    theta = np.random.uniform(-<span class="number">3</span>, <span class="number">3</span>, (X.shape[<span class="number">1</span>] + <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置参数</span></span><br><span class="line">    iterations = <span class="number">400</span></span><br><span class="line">    alpha = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度下降得到theta和损失值</span></span><br><span class="line">    theta, costs = gradientDescent(X, y, theta, iterations, alpha)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;theta:\n&quot;</span>, theta)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 画数据散点图和拟合直线（只有一个特征时可用）</span></span><br><span class="line">    plt.scatter(X, y)</span><br><span class="line">    h_theta = theta[<span class="number">0</span>] + theta[<span class="number">1</span>] * X</span><br><span class="line">    plt.plot(X, h_theta, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 画损失函数图</span></span><br><span class="line">    <span class="comment"># 生成从1到iterations的iterations个数字</span></span><br><span class="line">    x_axis = np.linspace(<span class="number">1</span>, iterations, iterations)</span><br><span class="line">    <span class="comment"># 将iterations个数字作为横坐标，iterations个损失值作为纵坐标</span></span><br><span class="line">    plt.plot(x_axis, costs[<span class="number">0</span>:iterations])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(predict(([[<span class="number">5.734</span>], [<span class="number">2.425</span>]]), x_average, sigma, theta))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对模型进行评价</span></span><br><span class="line">    model_predict = predict(X_original, x_average, sigma, theta)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;mse&#x27;</span>, mse(y, model_predict))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;rmse&#x27;</span>, rmse(y, model_predict))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;mae&#x27;</span>, mae(y, model_predict))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<h2 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h2><h3 id="概念："><a href="#概念：" class="headerlink" title="概念："></a>概念：</h3><p><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/19.png"></p>
<p>Good Fit&#x2F;Robust(鲁棒性)：模型的复杂度足够，能够很好地拟合训练数据，即模型的训练误差和测试误差都很小</p>
<p>Underfitting(欠拟合)：模型的复杂度不够，不能很好地拟合训练数据，即模型的训练误差很小，但测试误差很大</p>
<p>Overfitting(过拟合)：模型的复杂度过高，拟合了噪声，即模型的训练误差很小，但测试误差很大</p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>正则化用于防止过拟合，通过限制模型的复杂度来减少模型的自由度，使模型更健壮</p>
<p>lambda是正则化参数，需要根据实际情况进行调整</p>
<ol>
<li>L2 范数正则化（Ridge Regression，岭回归）<br>目标函数：<br><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/20.png"><br>代码：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradientDescent_ridge</span>(<span class="params">X, y, theta, iterations, alpha, lamda=<span class="number">0.001</span></span>):</span><br><span class="line">    c = np.ones(X.shape[<span class="number">0</span>]).transpose() <span class="comment"># 生成与X行数相等的1,并转置变为列向量</span></span><br><span class="line">    X = np.insert(X, <span class="number">0</span>, values = c, axis = <span class="number">1</span>) <span class="comment"># 在横轴第0个位置插入c</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>] <span class="comment"># 行数,数据量</span></span><br><span class="line">    n = X.shape[<span class="number">1</span>] <span class="comment"># 列数, 特征数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存每次迭代的损失</span></span><br><span class="line">    costs = np.zeros(iterations)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            theta[j] = (theta[j] + (alpha / m) * np.<span class="built_in">sum</span>((y - np.dot(X, theta)) * X[:, j].reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">                        - <span class="number">2</span> * lamda * theta[j])  <span class="comment"># 加上这一项就是ridge</span></span><br><span class="line">        costs[num] = computeCost(X, y, theta)</span><br><span class="line">    <span class="comment"># 返回theta和损失值</span></span><br><span class="line">    <span class="keyword">return</span> theta, costs</span><br></pre></td></tr></table></figure></li>
<li>L1 范数正则化（LASSO 回归）<br>目标函数：<br><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/21.png"><br>求解：<br>对θj求偏导，新加的正则化项求导后只剩下2*θj<br><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/23.png"></li>
<li>L1 与 L2 结合（Elastic Net，弹性网）<br>目标函数：<br><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/22.png"><br>ρ是弹性系数，取值范围为[0,1]，0表示 L2 正则化，1表示 L1 正则化，0.5表示 L1 与 L2 正则化的折中</li>
</ol>
<h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><h3 id="最小二乘法求一元线性回归"><a href="#最小二乘法求一元线性回归" class="headerlink" title="最小二乘法求一元线性回归"></a>最小二乘法求一元线性回归</h3><p>设线性回归方程: f(x) &#x3D; wx + b，其中w为回归系数，b为截距</p>
<p>公式如下：<br><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/24.png"><br><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/25.png"><br><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/26.png"><br><img src="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/images/27.png"></p>
<p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">data = np.loadtxt(<span class="string">&#x27;data/data1.txt&#x27;</span>, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分离x和y值</span></span><br><span class="line">x = data[:, <span class="number">0</span>]</span><br><span class="line">y = data[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算x和y的均值</span></span><br><span class="line">mean_x = np.mean(x, axis = <span class="number">0</span>)</span><br><span class="line">mean_y = np.mean(y, axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># xd和yd向量</span></span><br><span class="line">xd = x - mean_x</span><br><span class="line">yd = y - mean_y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算w</span></span><br><span class="line">w = np.dot(xd, yd) / np.dot(xd, xd)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;w = &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(w))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算b</span></span><br><span class="line">b = mean_y - w * mean_x</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b = &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(b))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得出线性回归方程</span></span><br><span class="line">f = w * x + b</span><br><span class="line"><span class="keyword">if</span> b &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;f(x) = &#123;:.4f&#125;x + &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(w, b))</span><br><span class="line"><span class="keyword">elif</span> b &lt; <span class="number">0</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;f(x) = &#123;:.4f&#125;x - &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(w, <span class="built_in">abs</span>(b)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 判定系数r² r_square = 1 - (残差平方和RSS / 总平方和TSS)</span></span><br><span class="line">RSS = np.<span class="built_in">sum</span>((y - f) ** <span class="number">2</span>)</span><br><span class="line">TSS = np.<span class="built_in">sum</span>((y - mean_y) ** <span class="number">2</span>)</span><br><span class="line">r_square = <span class="number">1</span> - (RSS / TSS)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;r² = &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(r_square))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建图形</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制散点图</span></span><br><span class="line">plt.scatter(x, y, color=<span class="string">&#x27;blue&#x27;</span>, alpha=<span class="number">0.8</span>, label=<span class="string">&#x27;Data Points&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制回归直线</span></span><br><span class="line">x_line = np.linspace(<span class="built_in">min</span>(x), <span class="built_in">max</span>(x))</span><br><span class="line">y_line = w * x_line + b</span><br><span class="line">plt.plot(x_line, y_line, color=<span class="string">&#x27;red&#x27;</span>, linewidth=<span class="number">2</span>, label=<span class="string">&#x27;Regression Line&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加标题和标签</span></span><br><span class="line"><span class="keyword">if</span> b &gt; <span class="number">0</span>:</span><br><span class="line">    plt.title(<span class="string">&#x27;f(x) = &#123;:.4f&#125;x + &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(w, b), fontsize=<span class="number">16</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    plt.title(<span class="string">&#x27;f(x) = &#123;:.4f&#125;x - &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(w, <span class="built_in">abs</span>(b)), fontsize=<span class="number">16</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>, fontsize=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加图例</span></span><br><span class="line">plt.legend(fontsize=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加网格</span></span><br><span class="line">plt.grid(<span class="literal">True</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, alpha=<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加r²</span></span><br><span class="line">plt.text(<span class="number">0.05</span>, <span class="number">0.95</span>, <span class="string">f&#x27;r² = <span class="subst">&#123;r_square:<span class="number">.4</span>f&#125;</span>&#x27;</span>, transform=plt.gca().transAxes,</span><br><span class="line">         fontsize=<span class="number">16</span>, verticalalignment=<span class="string">&#x27;top&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图形</span></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1080</span>)</span><br><span class="line">x_new = np.random.uniform(<span class="built_in">min</span>(x), <span class="built_in">max</span>(x), <span class="number">30</span>)</span><br><span class="line">y_predict = w * x_new + b</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预测数据点:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_new)):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;x_new = &#123;:.4f&#125;, y_predict = &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(x_new[i], y_predict[i]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建图形</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制回归直线</span></span><br><span class="line">x_line = np.linspace(<span class="built_in">min</span>(x), <span class="built_in">max</span>(x), <span class="number">100</span>)</span><br><span class="line">y_line = w * x_line + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制f(x)</span></span><br><span class="line">plt.plot(x_line, y_line, color=<span class="string">&#x27;red&#x27;</span>, linewidth=<span class="number">2</span>, label=<span class="string">&#x27;Regression Line&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制预测点, zorder=5保证点绘制于线上方</span></span><br><span class="line">plt.scatter(x_new, y_predict, color=<span class="string">&#x27;green&#x27;</span>, label=<span class="string">&#x27;Predicted Points&#x27;</span>, zorder=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加标题和标签</span></span><br><span class="line"><span class="keyword">if</span> b &gt; <span class="number">0</span>:</span><br><span class="line">    plt.title(<span class="string">&#x27;Predicted Results: f(x) = &#123;:.4f&#125;x + &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(w, b), fontsize=<span class="number">16</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    plt.title(<span class="string">&#x27;Predicted Results: f(x) = &#123;:.4f&#125;x - &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(w, <span class="built_in">abs</span>(b)), fontsize=<span class="number">16</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>, fontsize=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加图例</span></span><br><span class="line">plt.legend(fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加网格</span></span><br><span class="line">plt.grid(<span class="literal">True</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, alpha=<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图形</span></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://example.com">Taohua</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://example.com/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">http://example.com/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/%E5%AD%A6%E6%A0%A1%E8%AF%BE%E7%A8%8B/">学校课程</a></div><div class="post-share"><div class="social-share" data-image="/img/music_cat.gif" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/11/04/CppPrimer/ch03/CppPrimerch03Exercise/" title="CppPrimerch03Exercise"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">CppPrimerch03Exercise</div></div><div class="info-2"><div class="info-item-1">github资源和源码仓库 第三章 字符串、向量和数组为了方便练习，新加四个shell脚本：四个脚本放在同一个目录，之后再配置到PATH中:打开~&#x2F;.bashrc文件，在最后添加以下内容： 12345678export PATH=&quot;$PATH:&lt;你保存这三个文件的目录路径&gt;&quot;#比如我是这样写：export PATH=&quot;$PATH:$HOME/code/CppPrimerExercise/scripts&quot;#然后输入下面命令来使配置生效source ~/.bashrc#然后输入下面命令，查看是否配置成功echo $PATH#这样就可以在任意文件夹使用它们啦  compile用法： 12sh compile &lt;cpp文件名&gt; # 一定要带.cpp后缀示例：sh compile exercise.cpp  run用法： 12sh run &lt;文件名&gt; # 可以不带任何后缀，也可以是.cpp或.exe文件 示例：sh run exercise 或 sh run exercise.cpp 或 sh run e...</div></div></div></a><a class="pagination-related" href="/2025/10/25/Qt/%E7%BB%83%E4%B9%A0/02/Qt%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E3%80%81%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E3%80%81json%E6%93%8D%E4%BD%9C%E7%BB%83%E4%B9%A0/" title="Qt文件操作、配置文件、json操作练习"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">Qt文件操作、配置文件、json操作练习</div></div><div class="info-2"><div class="info-item-1">Qt 文件操作、配置文件、json 操作练习所需头函数1234567891011#include &lt;QCoreApplication&gt;#include &lt;QDebug&gt;#include &lt;QFile&gt;#include &lt;QTextStream&gt;#include &lt;QFileInfo&gt;#include &lt;QSettings&gt;#include &lt;QJsonDocument&gt;#include &lt;QJsonObject&gt;#include &lt;QJsonArray&gt;#include &lt;QJsonValue&gt;#include &lt;QJsonParseError&gt;  文件操作写入文件12345678910111213141516171819202122232425262728293031323334353637383940// 写入文件void test_1()&#123;    // 使用流输出操作符&lt;&lt;的时候不用qPrintable()    // 使...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/music_cat.gif" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Taohua</div><div class="author-info-description">一名计算机科学与技术专业的大学牲</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">17</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/rujiantaohuakai"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/rujiantaohuakai" target="_blank" title="Github"><i class="fab fa-github" style="color: #e3e8f4;"></i></a><a class="social-icon" href="/m17731016540@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is Taohua's Blog! Welcome to you!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%A2%E5%BC%8F%E5%8C%96%E5%AE%9A%E4%B9%89"><span class="toc-number">1.1.</span> <span class="toc-text">形式化定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="toc-number">1.2.</span> <span class="toc-text">梯度下降算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">1.2.1.</span> <span class="toc-text">批量梯度下降法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">1.2.2.</span> <span class="toc-text">随机梯度下降法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">1.2.3.</span> <span class="toc-text">小批量梯度下降法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.3.</span> <span class="toc-text">梯度下降法求解线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">1.3.1.</span> <span class="toc-text">加载数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">1.3.2.</span> <span class="toc-text">特征归一化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E4%BB%A3%E4%BB%B7J-%CE%B8"><span class="toc-number">1.3.3.</span> <span class="toc-text">计算代价J(θ)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.4.</span> <span class="toc-text">梯度下降函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.5.</span> <span class="toc-text">预测函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.6.</span> <span class="toc-text">模型评价函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.7.</span> <span class="toc-text">主函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-number">1.4.</span> <span class="toc-text">过拟合和欠拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5%EF%BC%9A"><span class="toc-number">1.4.1.</span> <span class="toc-text">概念：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">1.4.2.</span> <span class="toc-text">正则化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95"><span class="toc-number">1.5.</span> <span class="toc-text">最小二乘法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.5.1.</span> <span class="toc-text">最小二乘法求一元线性回归</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/04/CppPrimer/ch03/CppPrimerch03Exercise/" title="CppPrimerch03Exercise">CppPrimerch03Exercise</a><time datetime="2025-11-04T07:45:28.000Z" title="Created 2025-11-04 15:45:28">2025-11-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" title="梯度下降与最小二乘法求解线性回归">梯度下降与最小二乘法求解线性回归</a><time datetime="2025-10-27T13:43:06.000Z" title="Created 2025-10-27 21:43:06">2025-10-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/25/Qt/%E7%BB%83%E4%B9%A0/02/Qt%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E3%80%81%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E3%80%81json%E6%93%8D%E4%BD%9C%E7%BB%83%E4%B9%A0/" title="Qt文件操作、配置文件、json操作练习">Qt文件操作、配置文件、json操作练习</a><time datetime="2025-10-25T14:23:49.000Z" title="Created 2025-10-25 22:23:49">2025-10-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/24/Qt/%E7%BB%83%E4%B9%A0/01Qt%E5%9F%BA%E7%A1%80%E6%96%87%E4%BB%B6%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/Qt%E5%9F%BA%E7%A1%80%E6%96%87%E4%BB%B6%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%BB%83%E4%B9%A0/" title="Qt基础文件与基本数据类型练习">Qt基础文件与基本数据类型练习</a><time datetime="2025-10-24T10:33:33.000Z" title="Created 2025-10-24 18:33:33">2025-10-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/21/CppPrimer/ch02/CppPrimerch02Exercise/" title="CppPrimerCh02Exercise">CppPrimerCh02Exercise</a><time datetime="2025-10-20T17:46:12.000Z" title="Created 2025-10-21 01:46:12">2025-10-21</time></div></div></div></div></div></div></main><footer id="footer" style="background: linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347);"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By Taohua</span></div><div class="footer_custom_text">The real talent is resolution aspirations.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>